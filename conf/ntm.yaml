defaults: # loads default configs
  - task: nlp/ntm 
  - optimizer: sgd
  - scheduler: reduce_lr_on_plateau
  - training: default
  - trainer: default
  - trainer/logger: wandb
  - hydra: output/custom
  - override dataset: nlp/ntm/copy_task

backbone:
  pretrained_model_name_or_path: bert-base-uncased

dataset:
  cfg:
    train_val_split: 0.01
    # history_delimeter: ' [SEP] '
    # history_size: 1
    # hierarchical: False
    seq_width: 8
    min_len: 1
    max_len: 20
    num_exs: 50000

task:
  ntm:
    num_inputs: 9
    num_outputs: 8
    controller_size: 100
    controller_layers: 1
    num_heads: 1
    N: 128
    M: 20
  cfg:
    pooling_method: cls
    task_name: ntm
    activation: relu1
    scheduler_monitor: val_loss
    scheduler_mode: min

trainer:
  gpus: 0
  val_check_interval: 1000
  max_steps: 50000
  max_epochs: 100
  gradient_clip_val: 0.1
  default_root_dir: ${oc.env:STORAGE}/trained/lit/${task.cfg.task_name}/${backbone.pretrained_model_name_or_path}_${dataset.cfg.pretrained_dataset_name}/${experiment_name}
  resume_from_checkpoint: ${trainer.default_root_dir}/last.ckpt
  logger:
    project: ntm

training:
  lr: 15
  batch_size: 1

optimizer:
  weight_decay: 0

experiment_name: ${now:%Y-%m-%d}_${now:%H-%M-%S-%f}
log: True
ignore_warnings: True # todo: check warnings before release
seed: 33
stage: train # choose from train, test, (predict, interact, to be added)
finetune_ckpt: null
