defaults: # loads default configs
  - task: nlp/language_modeling
  - optimizer: adam
  - scheduler: constant_schedule_with_warmup
  - training: default
  - trainer: default
  - trainer/logger: wandb
  - hydra: output/custom
  - override dataset: nlp/language_modeling/wikitext

backbone:
  pretrained_model_name_or_path: gpt2

dataset:
  cfg:
    eval_batch_size: 10
    block_size: 512
    stride: 256
    dataset_config_name: wikitext-103-raw-v1

task:
  # _target_: lightning_transformers.task.nlp.conversation.ConversationTransformer
  cfg:
    compute_generate_metrics: True
    pooling_method: cls
    task_name: lm_ct
    scheduler_monitor: val_loss
    scheduler_mode: min
    no_repeat_ngram_size: 0 # 3 by default
    encoder_no_repeat_ngram_size: 0 # 3 by default
    generation_length: 100
    min_length: 50
    num_beams: 1 # 10 by default
    lm_stride: ${dataset.cfg.stride}
    disparate: False # toggle for using the disparate regulariser
    padding_mask: True # mask for padding tokens
    identical_mask: False # mask for identical tokens
    disparate_alpha: 10 # coefficient for similarity loss
    strengthen_position: False # whether to strengthen positifon embedding at each decoder layer
    generate_after_progress: 0 # don't generate before this threshold, to save time
    sim_threshold: 0.1
    unlikelihood: False
    ct_seq: False
    topk_negatives: 0 # topk prediction as negative examples; 0: skipping
    preced_k_negatives: 0 # 0: use none; -1: use all; k: use preceding k tokens as negatives
    negative_method: cl2 # cl1, ul
    scratch: False
    save_generation_path: null # ${trainer.default_root_dir}/generated.txt
    clr: False # contrastive learning representation from SimCLR
    topk_positives: 2 # 2 also works
    neg_hardness: 5
    ct_window_size: 150
    # compute_generate_metrics: False

trainer:
  gpus: 1
  val_check_interval: 1000
  max_steps: 50000
  max_epochs: 100
  gradient_clip_val: 0.1
  resume_from_checkpoint: ${trainer.default_root_dir}/last.ckpt
  default_root_dir: ${oc.env:STORAGE}/trained/lit/${task.cfg.task_name}/${backbone.pretrained_model_name_or_path}_${dataset.cfg.pretrained_dataset_name}/${experiment_name}
  logger:
    project: ct
  callbacks:
    checkpoint_callback:
      save_last: True
      save_top_k: 1

training:
  lr: 1e-5
  batch_size: 4

optimizer:
  weight_decay: 0

experiment_name: ${now:%Y-%m-%d}_${now:%H-%M-%S-%f}
log: True
ignore_warnings: True # todo: check warnings before release
seed: 0
stage: train # choose from train, test, (predict, interact, to be added)
finetune_ckpt: null
