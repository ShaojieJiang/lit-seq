from functools import partial
from typing import Any, Callable, Optional, Tuple

from datasets import Dataset
from torch import nn
from transformers import PreTrainedTokenizerBase, default_data_collator

from lightning_transformers.core.nlp import HFDataModule
from lightning_transformers.core.nlp.seq2seq import Seq2SeqDataConfig
from lightning_transformers.core.utils import calc_nidf_on_labels


class Seq2SeqDataModule(HFDataModule):
    """Defines the ``LightningDataModule`` for Seq2Seq Datasets, such as Summarization and Translation.

    Args:
        *args: ``HFDataModule`` specific arguments.
        cfg: Contains data specific parameters when processing/loading the dataset
            (Default ``Seq2SeqDataConfig``)
        **kwargs: ``HFDataModule`` specific arguments.
    """

    cfg: Seq2SeqDataConfig

    def __init__(self, *args, cfg: Seq2SeqDataConfig = Seq2SeqDataConfig(), **kwargs) -> None:
        super().__init__(*args, cfg=cfg, **kwargs)

    def process_data(self, dataset: Dataset, stage: Optional[str] = None) -> Dataset:
        src_text_column_name, tgt_text_column_name = self.source_target_column_names

        convert_to_features = partial(
            self.convert_to_features,
            tokenizer=self.tokenizer,
            padding=self.cfg.padding,
            max_source_length=self.cfg.max_source_length,
            max_target_length=self.cfg.max_target_length,
            src_text_column_name=src_text_column_name,
            tgt_text_column_name=tgt_text_column_name,
        )
        dataset = dataset.map(
            convert_to_features,
            batched=True,
            num_proc=self.cfg.preprocessing_num_workers,
            load_from_cache_file=self.cfg.load_from_cache_file,
        )

        cols_to_keep = [x for x in ["input_ids", "attention_mask", "labels"] if x in dataset["train"].features]
        dataset.set_format(columns=cols_to_keep)
        nidf = calc_nidf_on_labels(dataset)
        nidf_emb = nn.Embedding(max(list(nidf.keys())) + 1, 1)
        nidf_emb.weight.requires_grad = False
        for id in range(len(nidf_emb.weight)):
            nidf_emb.weight[id] = nidf[id]
        for split in dataset.keys():
            dataset[split].nidf_emb = nidf_emb
        return dataset

    @property
    def source_target_column_names(self) -> Tuple[str, str]:
        return "source", "target"

    @staticmethod
    def convert_to_features(
        examples: Any,
        tokenizer: PreTrainedTokenizerBase,
        padding: str,
        max_source_length: int,
        max_target_length: int,
        src_text_column_name: str,
        tgt_text_column_name: str,
    ):
        encoded_results = tokenizer.prepare_seq2seq_batch(
            src_texts=examples[src_text_column_name],
            tgt_texts=examples[tgt_text_column_name],
            max_length=max_source_length,
            max_target_length=max_target_length,
            padding=padding,
        )
        return encoded_results

    @property
    def collate_fn(self) -> Callable:
        return default_data_collator
